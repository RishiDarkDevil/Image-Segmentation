---
title: "Image Segmentation"
author: "Rishi Dey Chowdhury"
date: "1/2/2022"
output: 
  html_document:
    keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(knitr)
library(kableExtra)
library(purrr)
library(viridis)
library(jpeg)
library(png)
options(dplyr.summarise.inform = FALSE)
```

# INTRODUCTION

**Image Segmentaion is to divide the image into disjoint homogeneous regions or classes, where all the pixels in the same class must have some common characterestics**. With recent growth in image data in various fields like Medical Imaging- MRI images, fMRI images, X-Ray images, PET Scans, CT Scans, Ultrasound; Facial Recognition; Handwritten Notes Scans, etc it becomes important to devise a method to segment parts of image to study specific ROI(Region of Interests) in MRI, etc.

**A Grayscale(Color) image, afterall, is nothing but a(a set of) matrix(matrices) i.e. single channel(3 channels -RGB) of intensity values(0 to 1, both inclusive)**. So, there are parts of images with higher intensity and parts with lower intensity, parts of images which share common intensity pattern. So our goal is to exploit this feature of image to segment or classify the pixels.

# IDEA & ALGORITHM

Image is thought of as a random process, particularly, a mixture of random processes.

### MODEL ASSUMPTIONS:
- **pixels(intensity values) of an image to be coming from a Gaussian Mixture Model(GMM)**.
- **The Gaussians in the mixture model are independent**.
- **pixels are iid sample from the above mentioned GMM**.
The GMM model's pdf is given by-
$$f(x_i|\Phi) = \sum_{k=1}^{K}p_k.G(x_i|\theta_k)$$
Where K is the nmber of processes or classes that need to be extracted from the image, $\theta_k\ \forall\ k=1,2,\dots, K$ is a parameter vector of class  and its form $[\mu_k,\ \sigma_k]$ such that $\mu_k,\ \sigma_k$ are mean and standard deviation of the distribution $k$, respectively, $p_k$ is mixing proportion of class $k$ ($0<p_k<1,\ \forall\ k=1,\dots,K$ and $\sum_kp_k=1$), $x_i$ is the intensity of pixel $i$, and $\Phi = \{p_1,\dots, p_k, \mu_1,\dots,\mu_k \}$ is called the parameters vector of the mixture.

The parameter vector $\Phi$ is the missing data that defines the mixture. The EM ALgorithm utilize the maximum log-likelihood estimator to estimate the value of $\Phi$. The likelihood is given by,
$$L(\Phi) = f(x_1,\dots,x_n|\Phi) = \prod_{i=1}^n f(x_i|\Phi)$$
We use the EM-Algorithm to solve for $\Phi_{ML}$, which maximizes the likelihood (otherwise computationally expensive(sometimes intractable)) i.e.,
$$L(\Phi_{ML}) = arg\ max\ (L(\Phi))$$
Hence, $\Phi_{ML}$ is called the maximum log-likelihood estimato of $\Phi$.

### EXPECTATION MAXIMIZATION ALGORITHM

Once the true value of the parameters are found then the class of each pixel in the image can be easily determined by computing the probability of this pixel to be belong to each class in the image. The EM algorithm is used to find the unknown parameters of a mixture model, that maximizes the log-likelihood of the sample.

- The **Expectation(E) Step**:
Compute the expected value of $z_{ik}$ use the current estimate of the parameter vector $\Phi$,
$$z_{ik}^{(t)} = \frac{p_k^{(t)}.G(x_i|\theta_k^{(t)})}{f(x_i|\Phi^{(t)})}$$
Where $z_{ik}$ is the posteriori probability that given $x_i,\ x_i$ comes from class k. The $N\times K$ matrix $Z=[z_{ik}]$ of posteriori probability satisfies the constraints, ($0\leq z_{ik}\leq 1, \sum_k z_{ik} = 1, \sum_i z_{ik}>0, 1\leq i\leq N, 1\leq k\leq K$). $x_i$ is the value of the pixel $i$. $G(x_i|\theta_k^{(t)})$ is the probability of pixel i given it is a member of class k.

- The **Maximization(M) Step**:
Using the Expectation Step Data as if it were actually measured data,
$$\mu_k^{(t+1)} = \frac{\sum_{i=1}^N z_{ik}^{(t)}.x_i}{\sum_{i=1}^N z_{ik}^{(t)}}$$
$${\sigma^2_{k}}^{(t+1)} = \frac{\sum_{i=1}^N z_{ik}^{(t)}.(x_i-\mu^{(t+1)})^2}{\sum_{i=1}^{N} z_{ik}^{(t)}}$$
$$p_k^{(t+1)} = \frac{\sum_{i=1}^N z_{ik}^{(t)}}{N}$$
We keep on iterating the above two steps until the parameter estimates change in an iteration falls below some tolerance level or when NaN comes in the optimization after optimizing implying no further numerical optimization can be done.

- The **Classification Step**:
After we have applied EM-Algorithm and we have the parameter estimate, we use it to find the probability(density) at each point belonging to each of the $K$ class. Then, we classify the point to be belonging to the class which attains the maximum probability(density) for that point out of the $K$ classes.

# APPLICATION

Here we will apply the above described algorithm on some Brain MRI Images, etc.
- We have the following Brain MRI Image and taking a look at it we can visually see 3 different classes is the most probable i.e. the background, the CSF and the gray matter.
```{r segment_function}
# Returns the segmented image matrix given an image
# image.name = ?, number of gaussians to be used(Default: 2), number of iterations(Default: 10), tolerance(Default: 1e-3)
Segment.Image.EM <- function(image.name, n.gaussians = 2, niter = 10, eps = 1e-3){
  
  if(class(image.name) == "array"){
    # Assigning the image matrix
    img <- image.name
  }else if (substr(image.name, str_length(image.name)-2, str_length(image.name)) == 'png') {
    # Required Package
    require(png)
    # Reading the image
    img <- readPNG(image.name)
  }  else{
    # Required Package
    require(jpeg)
    # Reading the image
    img <- readJPEG(image.name)
  }
  
  # Seeing the image matrix
  #print("Image Info:")
  if(length(dim(img)) == 3)
    img <- img[ , ,1]
  #print(dim(img))
  
  # Seeing the image
  #grid::grid.raster(img)
  
  # I am assuming that the data is generated from n.gaussians number of clusters(Normal Clusters)
  
  mu <- floor((-n.gaussians/2)+1):floor(n.gaussians/2) 
  sigma <- rep(1, n.gaussians)
  p <- rep(1/n.gaussians, n.gaussians)
  
  # Creating a matrix of N x n.gaussians size where N = dim(img)[1] x dim(img)[2](i.e. total number of pixels) and n.gaussians cause that's the probability that a particular pixel belongs
  # to a particular cluster
  prob.mat <- matrix(rep(0, dim(img)[1]*dim(img)[2]*n.gaussians), nrow = dim(img)[1]*dim(img)[2], ncol = n.gaussians)
  
  for(iter in 1:niter){
    
    # Expectation Step
    k <- 1
    for(j in 1:dim(img)[2]){
      for (i in 1:dim(img)[1]) {
        deno.prob <- 0
        num.prob.r <- c()
        for (r in 1:n.gaussians) {
          num.prob.r <- c(num.prob.r, p[r]*dnorm(img[i,j], mu[r], sigma[r]))
          deno.prob <- deno.prob + num.prob.r[r]
        }
        prob.mat[k, ] <- num.prob.r / deno.prob
        k <- k+1
      }
    }
    mu.o <<- mu
    sigma.o <<- sigma
    p.o <<- p
    
    # Maximization Step
    mu <- colSums(prob.mat*c(img)) / colSums(prob.mat)
    sigma <- sqrt(colSums(prob.mat*(sweep(matrix(rep(c(img), n.gaussians), ncol = n.gaussians), 2, mu))^2)/colSums((prob.mat)))
    p <- colSums(prob.mat)/(dim(img)[1]*dim(img)[2])
    
    #cat('iter:', iter, '\n', 'mu:', mu, '\n', 'sigma:', sigma, '\n', 'probs:', p, '\n')
    
    # Stops if the L_inf norm falls below tolerance or any of the param becomes NaN
    if((max(abs(mu.o - mu), abs(sigma.o-sigma), abs(p.o-p)) < eps) | NaN %in% mu | NaN %in% sigma | NaN %in% p)
      break
    if(iter == niter)
      print("Max. Iteration Limit Exceeded!")
  }
  
  # Visualizing the segmentation
  seg.img <<- matrix(rep(c(img), n.gaussians), ncol = n.gaussians)
  seg.img[prob.mat < 1/n.gaussians] = 1
  segmented.image <<- list()
  
  for (i in 1:n.gaussians) {
    segmented.image[[paste('seg.img.mat.',i,sep='')]] <<- matrix(seg.img[,i], nrow = dim(img)[1], ncol = dim(img)[2])
  }
  return(segmented.image)
}
```

```{r brain_image, cache=TRUE, fig,asp=1, fig.width=12}
brain.img <- readJPEG("brainMRI.jpg")
plot(1:2, type = "n")
rasterImage(brain.img, 1,1, 2,2)
```

-  We run the algorithm with K=3(3 clusters), tolerance of $10^{-3}$, maximum iteration of 4000. We get convergence as per the tolerance criteria, below we show the estimated Normals and their proportions as found by EM-Algorithm. Just by glancing we see that the proportion of the black pixels which is supposed to be coming from the degenerate normal model is 64%, and since most of the MRI Scan is black it seems okay. 
```{r run_func_brain, cache=TRUE}
brain.seg <- Segment.Image.EM("brainMRI.jpg", 3,  niter = 4000)
```

```{r brain_gaussian, fig.asp=1, fig.width=12}
x <- seq(min(mu.o)-5*max(sigma.o), max(mu.o)+5*max(sigma.o), by = 0.001)
my_data <- tibble(mean = mu.o, stdev = sigma.o, test = as.character(1:length(mu.o)))
tall_df2 <- pmap_df(my_data, ~data_frame(x = x, test = ..3, density = dnorm(x, ..1, ..2)))

p <- ggplot(tall_df2, aes(color = factor(test), x = x, y = density)) +
  geom_line() +
  geom_segment(data = my_data, aes(color = test, x = mean, y = 0, xend = mean, yend = 100), linetype = "dashed") + 
  coord_cartesian(ylim = c(0,3))  +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), strip.background = element_blank()) +
  theme(legend.position = "bottom") +
  labs(color = "Gaussian Models") +
  guides(
    fill = guide_legend(
      nrow = 1
    )
  ) + ggtitle("The Gaussians Estimated From Image")

t <- ggtexttable(tibble(mean = round(mu.o, digits = 2), stdev = round(sigma.o, digits = 2), props = round(p.o, digits = 2)), rows = NULL, theme = ttheme("mVioletWhite")) %>% 
  tab_add_title("Gaussian Parameters", face = 'bold')

ggarrange(p, t, ncol = 2, widths = c(8,2))
```
- Now, coming to the most interesting part that is seeing the result of the segmentation. The Segmentation works almost perfectly and separates the required 3 parts without any manual intervention apart from deciding on the number of clusters, tolerance and iteration limit.
```{r brain_segemented, fig.asp=1, fig.width=12}
par(mfrow = c(2,2))
plot(1:2, type = "n", main = "Original MRI Scan", xlab = "X", ylab = "Y")
rasterImage(brain.img, 1,1, 2,2)
plot(1:2, type = "n", main = "Background Separated", xlab = "X", ylab = "Y")
rasterImage(brain.seg$seg.img.mat.1, 1,1, 2,2)
plot(1:2, type = "n", main = "Grey Matter Separated", xlab = "X", ylab = "Y")
rasterImage(brain.seg$seg.img.mat.2, 1,1, 2,2)
plot(1:2, type = "n", main = "White Matter Separated", xlab = "X", ylab = "Y")
rasterImage(brain.seg$seg.img.mat.3, 1,1, 2,2)
par(mfrow = c(1,1))
```
- Let's Apply our Segmentation function on some other images, to see how well it works. One possible application of this technique can be to scan handwritten notes, sketches, etc. and from below it looks like a perfectly scanned note(We obv. used 2 clusters).
```{r handwritten_run_func, cache=TRUE}
handwritten.img <- readJPEG("handwritten.jpg")
suppressMessages(handwritten.seg <- Segment.Image.EM("handwritten.jpg",  niter = 20))
```

```{r handwritten_segment, fig,asp=1, fig.width=12}
par(mfrow = c(1,2))
plot(1:2, type = "n", main = "Original Handwritten Note", xlab = "X", ylab = "Y")
rasterImage(handwritten.img, 1,1, 2,2)
scan <- handwritten.seg$seg.img.mat.1
scan[scan != 1] <- 0
scan[,1:10] <- 1
plot(1:2, type = "n", main = "Converted Scanned Note", xlab = "X", ylab = "Y")
rasterImage(scan, 1,1, 2,2)
par(mfrow = c(1,1))
```

# CONCLUSION

We used EM-Algorithm to Segment Image into sections which have common characteristics. This has the following advantages:
- It is computationally comparatively fast.
- Running few iterations works fine for most cases.
- Can use any number of clusters.
- Helps in study of ROI of an image(specially medical images).

Drawbacks:
- It models pixels as iid samples rather than taking advantage of the spatial correlation.
- Have to choose the number of clusters wisely.
- Produces granular segmented images sometimes(Due to the 1st Drawback).


