---
title: "Image Segmentation"
author: "Rishi Dey Chowdhury"
date: "1/2/2022"
output: 
  html_document:
    keep_md: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(knitr)
library(kableExtra)
library(purrr)
library(viridis)
library(jpeg)
library(png)
options(dplyr.summarise.inform = FALSE)
```

# INTRODUCTION

**Image Segmentaion is to divide the image into disjoint homogeneous regions or classes, where all the pixels in the same class must have some common characterestics**. With recent growth in image data in various fields like Medical Imaging- MRI images, fMRI images, X-Ray images, PET Scans, CT Scans, Ultrasound; Facial Recognition; Handwritten Notes Scans, etc it becomes important to devise a method to segment parts of image to study specific ROI(Region of Interests) in MRI, etc.

**A Grayscale(Color) image, afterall, is nothing but a(a set of) matrix(matrices) i.e. single channel(3 channels -RGB) of intensity values(0 to 1, both inclusive)**. So, there are parts of images with higher intensity and parts with lower intensity, parts of images which share common intensity pattern. So our goal is to exploit this feature of image to segment or classify the pixels.

# IDEA & ALGORITHM

Image is thought of as a random process, particularly, a mixture of random processes.

### MODEL ASSUMPTIONS:
- **pixels(intensity values) of an image to be coming from a Gaussian Mixture Model(GMM)**.
- **The Gaussians in the mixture model are independent**.
- **pixels are iid sample from the above mentioned GMM**.
The GMM model's pdf is given by-
$$f(x_i|\Phi) = \sum_{k=1}^{K}p_k.G(x_i|\theta_k)$$
Where K is the nmber of processes or classes that need to be extracted from the image, $\theta_k\ \forall\ k=1,2,\dots, K$ is a parameter vector of class  and its form $[\mu_k,\ \sigma_k]$ such that $\mu_k,\ \sigma_k$ are mean and standard deviation of the distribution $k$, respectively, $p_k$ is mixing proportion of class $k$ ($0<p_k<1,\ \forall\ k=1,\dots,K$ and $\sum_kp_k=1$), $x_i$ is the intensity of pixel $i$, and $\Phi = \{p_1,\dots, p_k, \mu_1,\dots,\mu_k \}$ is called the parameters vector of the mixture.

The parameter vector $\Phi$ is the missing data that defines the mixture. The EM ALgorithm utilize the maximum log-likelihood estimator to estimate the value of $\Phi$. The likelihood is given by,
$$L(\Phi) = f(x_1,\dots,x_n|\Phi) = \prod_{i=1}^n f(x_i|\Phi)$$
We use the EM-Algorithm to solve for $\Phi_{ML}$, which maximizes the likelihood (otherwise computationally expensive(sometimes intractable)) i.e.,
$$L(\Phi_{ML}) = arg\ max\ (L(\Phi))$$
Hence, $\Phi_{ML}$ is called the maximum log-likelihood estimato of $\Phi$.

### EXPECTATION MAXIMIZATION ALGORITHM

Once the true value of the parameters are found then the class of each pixel in the image can be easily determined by computing the probability of this pixel to be belong to each class in the image. The EM algorithm is used to find the unknown parameters of a mixture model, that maximizes the log-likelihood of the sample.

- The **Expectation(E) Step**:
Compute the expected value of $z_{ik}$ use the current estimate of the parameter vector $\Phi$,
$$z_{ik}^{(t)} = \frac{p_k^{(t)}.G(x_i|\theta_k^{(t)})}{f(x_i|\Phi^{(t)})}$$
Where $z_{ik}$ is the posteriori probability that given $x_i,\ x_i$ comes from class k. The $N\times K$ matrix $Z=[z_{ik}]$ of posteriori probability satisfies the constraints, ($0\leq z_{ik}\leq 1, \sum_k z_{ik} = 1, \sum_i z_{ik}>0, 1\leq i\leq N, 1\leq k\leq K$). $x_i$ is the value of the pixel $i$. $G(x_i|\theta_k^{(t)})$ is the probability of pixel i given it is a member of class k.

- The **Maximization(M) Step**:
Using the Expectation Step Data as if it were actually measured data,
$$\mu_k^{(t+1)} = \frac{\sum_{i=1}^N z_{ik}^{(t)}.x_i}{\sum_{i=1}^N z_{ik}^{(t)}}$$
$${\sigma^2_{k}}^{(t+1)} = \frac{\sum_{i=1}^N z_{ik}^{(t)}.(x_i-\mu^{(t+1)})^2}{\sum_{i=1}^{N} z_{ik}^{(t)}}$$
$$p_k^{(t+1)} = \frac{\sum_{i=1}^N z_{ik}^{(t)}}{N}$$
We keep on iterating the above two steps until the parameter estimates change in an iteration falls below some tolerance level or when NaN comes in the optimization after optimizing implying no further numerical optimization can be done.

- The **Classification Step**:
After we have applied EM-Algorithm and we have the parameter estimate, we use it to find the probability(density) at each point belonging to each of the $K$ class. Then, we classify the point to be belonging to the class which attains the maximum probability(density) for that point out of the $K$ classes.

# APPLICATION

Here we will apply the above described algorithm on some Brain MRI Images, etc.
- We have the following Brain MRI Image and taking a look at it we can visually see 3 different classes is the most probable i.e. the background, the CSF and the gray matter. So, we run the algorithm with K=3(3 clusters), tolerance of $10^{-3}$, maximum iteration of 100. 
```{r segment_function}
# Returns the segmented image matrix given an image
# image.name = ?, number of gaussians to be used(Default: 2), number of iterations(Default: 10), tolerance(Default: 1e-3)
Segment.Image.EM <- function(image.name, n.gaussians = 2, niter = 10, eps = 1e-3){
  
  if (substr(image.name, str_length(image.name)-2, str_length(image.name)) == 'png') {
    # Required Package
    require(png)
    # Reading the image
    img <- readPNG(image.name)
  }
  else{
    # Required Package
    require(jpeg)
    # Reading the image
    img <- readJPEG(image.name)
  }
  
  # Seeing the image matrix
  #print("Image Info:")
  if(length(dim(img)) == 3)
    img <- img[ , ,1]
  #print(dim(img))
  
  # Seeing the image
  grid::grid.raster(img)
  
  # I am assuming that the data is generated from n.gaussians number of clusters(Normal Clusters)
  
  mu <- floor((-n.gaussians/2)+1):floor(n.gaussians/2) 
  sigma <- rep(1, n.gaussians)
  p <- rep(1/n.gaussians, n.gaussians)
  
  # Creating a matrix of N x n.gaussians size where N = dim(img)[1] x dim(img)[2](i.e. total number of pixels) and n.gaussians cause that's the probability that a particular pixel belongs
  # to a particular cluster
  prob.mat <- matrix(rep(0, dim(img)[1]*dim(img)[2]*n.gaussians), nrow = dim(img)[1]*dim(img)[2], ncol = n.gaussians)
  
  for(iter in 1:niter){
    
    # Expectation Step
    k <- 1
    for(j in 1:dim(img)[2]){
      for (i in 1:dim(img)[1]) {
        deno.prob <- 0
        num.prob.r <- c()
        for (r in 1:n.gaussians) {
          num.prob.r <- c(num.prob.r, p[r]*dnorm(img[i,j], mu[r], sigma[r]))
          deno.prob <- deno.prob + num.prob.r[r]
        }
        prob.mat[k, ] <- num.prob.r / deno.prob
        k <- k+1
      }
    }
    mu.o <<- mu
    sigma.o <<- sigma
    p.o <<- p
    
    # Maximization Step
    mu <- colSums(prob.mat*c(img)) / colSums(prob.mat)
    sigma <- sqrt(colSums(prob.mat*(sweep(matrix(rep(c(img), n.gaussians), ncol = n.gaussians), 2, mu))^2)/colSums((prob.mat)))
    p <- colSums(prob.mat)/(dim(img)[1]*dim(img)[2])
    
    #cat('iter:', iter, '\n', 'mu:', mu, '\n', 'sigma:', sigma, '\n', 'probs:', p, '\n')
    
    # Stops if the L_inf norm falls below tolerance or any of the param becomes NaN
    if((max(abs(mu.o - mu), abs(sigma.o-sigma), abs(p.o-p)) < eps) | NaN %in% mu | NaN %in% sigma | NaN %in% p)
      break
    if(iter == niter)
      print("Max. Iteration Limit Exceeded!")
  }
  
  # Visualizing the segmentation
  seg.img <<- matrix(rep(c(img), n.gaussians), ncol = n.gaussians)
  seg.img[prob.mat < 1/n.gaussians] = 1
  segmented.image <<- list()
  
  for (i in 1:n.gaussians) {
    segmented.image[[paste('seg.img.mat.',i,sep='')]] <<- matrix(seg.img[,i], nrow = dim(img)[1], ncol = dim(img)[2])
  }
  return(segmented.image)
}
```

```{r brain_image, cache=TRUE, fig,asp=1, fig.width=12}
brain.img <- 
```

```{r brain_gaussian, fig.asp=1, fig.width=12}
x <- seq(min(mu.o)-5*max(sigma.o), max(mu.o)+5*max(sigma.o), by = 0.001)
my_data <- tibble(mean = mu.o, stdev = sigma.o, test = as.character(1:length(mu.o)))
tall_df2 <- pmap_df(my_data, ~data_frame(x = x, test = ..3, density = dnorm(x, ..1, ..2)))

p <- ggplot(tall_df2, aes(color = factor(test), x = x, y = density)) +
  geom_line() +
  geom_segment(data = my_data, aes(color = test, x = mean, y = 0, xend = mean, yend = 100), linetype = "dashed") + 
  coord_cartesian(ylim = c(0,3))  +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), strip.background = element_blank()) +
  theme(legend.position = "bottom") +
  labs(color = "Gaussian Models") +
  guides(
    fill = guide_legend(
      nrow = 1
    )
  ) + ggtitle("The Gaussians Estimated From Image")

t <- ggtexttable(tibble(props = round(p.o, digits = 2)), rows = NULL, theme = ttheme("mVioletWhite")) %>% 
  tab_add_title("Props", face = 'bold')

ggarrange(p, t, ncol = 2, widths = c(9,1))
```


